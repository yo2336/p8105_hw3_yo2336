---
title: "p8105_hw3_yo2336"
author: "Yoo Rim Oh"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
```

# Problem 1

## Load instacart data and description

Use data() to load data from p8105.datasets
```{r}
data("instacart")

nrow(instacart)
ncol(instacart)
names(instacart)

```

The instacart data has 15 variables representing product names, departments and aisles 
each product is from, and details on the orders. It also has 1,384,617 total observations 
for each products ordered. 


## Total aisle and description
```{r}
range(pull(instacart, aisle_id))
count(instacart, aisle_id, aisle) %>%
  arrange(desc(n))
```

There are 134 aisles total and top three aisles where the most products ordered came from 
were fresh vegetables, fresh fruits, and packaged vegetables fruits in order.


## Aisle vs product plot

Filtered to only have aisle data with more than 10,000 products ordered. Then created a plot
for how many products have been ordered from each aisle.
```{r}
aisle_df =
  instacart %>%
  count(aisle_id, name = "number_ordered") %>%
  filter(number_ordered > 10000)
  
aisle_df %>%  
  ggplot(aes(x = aisle_id, y = number_ordered)) +
  geom_point() +
  labs(title = "Products Ordered from Each Aisle with at least 10,000 Purchased") +
  scale_x_continuous(
    breaks = c(0, 25, 50, 75, 100, 125))
```

There were two aisles with significantly higher purchased products with approximately over 
140,000 counts. However, generally most aisles had under 40,000 purchased products.

## Popular items per aisle

Created data frame for each aisle, baking ingredient, dog food care, and packaged vegetables fruits.
Filtered to aisle specific data and counted how many times each products were ordered. Arranged 
data in descending order to get top 3 popularly ordered products. Then joined all three data frames 
and created a table representing the data.
```{r}
bake_ingr_df =
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "baking ingredients") %>%
  count(aisle, product_name, name = "count_baking") %>%
  arrange(desc(count_baking)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "baking ingredients")

dog_food_care_df = 
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "dog food care") %>%
  count(aisle, product_name, name = "count_dog") %>%
  arrange(desc(count_dog)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "dog food care")

pack_veg_fruit_df = 
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "packaged vegetables fruits") %>%
  count(aisle, product_name, name = "count_packaged") %>%
  arrange(desc(count_packaged)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "packaged vegetables fruits")

popular_table_df = 
  left_join(bake_ingr_df, left_join(dog_food_care_df, pack_veg_fruit_df, by = "rank"), 
            by = "rank") %>%
  janitor::clean_names()

knitr::kable(popular_table_df, caption = "Top 3 Products Ordered from Selected Aisles")
```

The top three products purchased from the baking ingredients were light brown sugar, 
pure baking soda, and cane sugar. The top three products purchased from the dog food 
care aisle were snack sticks chicken & rice recipe dog treats, organix chicken & brown 
rice recipe, and small dog biscuits. Finally the top three most purchased products from 
the packaged vegetables fruits were organic baby spinach, organic raspberries, and 
organic blueberries. The counts for the top three products purchased from the packaged 
vegetables fruits aisle were significantly higher as compared to the other two aisles.


## Mean hour a product was ordered each day

Filtered data for each product, pink lady apple and coffee ice cream. Grouped filtered data 
based on day of the week (0 to 6, what specific day each number represents not specified) 
and created new variable for mean hour each product was ordered. Then created a table for each 
pink lady apple and coffee ice cream to represent the data.
```{r}
pink_lady_apple =
  instacart %>%
  select("day" = "order_dow", order_hour_of_day, product_name) %>%
  filter(product_name == "Pink Lady Apples") %>%
  group_by(day) %>%
  summarize(mean_order_hour = mean(order_hour_of_day))
knitr::kable(pink_lady_apple, caption = "Mean Hour of Day Pink Lady Apples are Ordered")

coffee_ice_cream =
  instacart %>%
  select("day" = "order_dow", order_hour_of_day, product_name) %>%
  filter(product_name == "Coffee Ice Cream") %>%
  group_by(day) %>%
  summarize(mean_order_hour = mean(order_hour_of_day))
knitr::kable(coffee_ice_cream, caption = "Mean Hour of Day Coffee Ice Creams are Ordered")
```

The mean hour of the day coffee ice creams were ordered were generally later than the pink 
lady apple purchases. 

# Problem 2

## Load BRFSS data
```{r}
data(brfss_smart2010)
```

## Clean BRFSS

Filtered data to only see overall health topics. Reassigned the 5 responses that goes from poor to
excellent to be an ordered factors.
```{r}
overall_health =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"),
                           ordered = TRUE)) %>%
  rename("state" = "locationabbr", "county" = "locationdesc")
```


## States with >= 7 observed locations in 2002 and 2010

Filtered data to year specific, 2002 and 2010 each. Counted how many observations each state had 
but divided results by 5 since observations from each county were divided based on the 5 
responses. Then filtered data to only see states with at least 7 locations.
```{r}
nrow(
overall_health %>%
  filter(year == 2002) %>%
  count(state) %>%
  mutate(n = n/5) %>%
  filter(n >= 7))

nrow(
  overall_health %>%
  filter(year == 2010) %>%
  count(state) %>%
  mutate(n = n/5) %>%
  filter(n >= 7))
```

In 2002, 6 states observed 7 or more locations.
In 2010, 14 states observed 7 or more locations.

## Excellent response data frame and plot

Filtered data to only show observations with response excellent. Added new variable to 
represent mean data value across different locations within the states. Then created a plot
to observe to pattern for each state.
```{r}
excellent_df =
  overall_health %>%
  filter(response == "Excellent") %>%
  select(year, state, data_value) %>%
  group_by(year, state) %>%
  summarize(mean_data_value = mean(data_value)) %>%
  drop_na(mean_data_value)

excellent_df %>%
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_point() +
  geom_line() +
  labs(title = "Mean Excellent Responses Across Different Locations Within Each State")
```

The mean values from 2002 to 2010 generally decreased very slightly over time for each states.
With some exceptions, the mean values range from 15 to 30 approximately.

## Plot for NY responses to overall health in 2006 and 2010

Filtered to see data for New York state only and for years 2006 and 2010. Then created a plot 
showing the distribution of the responses per year.
```{r}
overall_health %>%
  filter(state == "NY", year %in% c(2006, 2010)) %>%
  select(year, state, response, data_value) %>%
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
  labs(title = "Distribution of Responses to Overall Health in NY")
```

Generally, there were more observations for the responses good and very good during the year 2010
compared to 2006.There were less lower level responses, poor and fair, observed as compared to 
higher responses for both years.

# Problem 3

## Load and clean data

Loaded csv data and cleaned. Added new variable to assign whether it was a weekday or weekend. 
```{r}
accel_df = 
  read_csv("accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(week_type = case_when(
    day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
    day %in% c("Saturday", "Sunday") ~ "weekend")) %>%
  relocate(week, day_id, day, week_type)

nrow(accel_df)
ncol(accel_df)

```

The accelerometer data has total of 1444 variables for week, day, and activity per minute 
(total 1440 for each minute) the 24 hour period each observed day. There are 35 total observations,
total 35 days observed.

## Total activity for each day

Obtained sum across all minute mark activity after grouping by day. Then created a table to 
represent the data.
```{r}
total_activity_df = 
  accel_df %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(across(starts_with("activity"))))
knitr::kable(total_activity_df, caption = "Total Activity for Each Day")
```

When looking at the table, there is no obvious pattern but potentially lower 
activity counts during some of the weekend days.

## Plot of activity over 24 hour each day

Used the sums across all 1440 activity interval and created a graph observing total activity 
over all 35 observed days and color coded by day of the week.
```{r}
accel_df %>%
  group_by(day_id, day) %>%
  summarize(total_activity = sum(across(starts_with("activity")))) %>%
  ggplot(aes(x = day_id, y = total_activity, color = day)) +
  geom_point() +
  geom_line() +
  labs(title = "24-hour Activity for Each Day",
       x = "Day",
       y = "Total Activity")
```

The observations for Saturdays eventually decreased to have the lowest values over the entire
35 observed days. Though values decreased soon after a peak, the third Monday had the highest
total activity count. Sundays had overall decreased activity count over the course of 35 days 
while Tuesdays and Wednesdays had consistent total counts over 35 days.