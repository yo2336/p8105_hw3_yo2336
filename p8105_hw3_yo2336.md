p8105\_hw3\_yo2336
================
Yoo Rim Oh

# Problem 1

## Load instacart data and description

Use data() to load data from p8105.datasets

``` r
data("instacart")

nrow(instacart)
```

    ## [1] 1384617

``` r
ncol(instacart)
```

    ## [1] 15

The instacart data has 15 variables representing product names,
departments and aisles each product is from, and details on the orders.
It also has 1,384,617 total observations for each products ordered.

## Total aisle and description

``` r
range(pull(instacart, aisle_id))
```

    ## [1]   1 134

``` r
count(instacart, aisle_id, aisle) %>%
  arrange(desc(n))
```

    ## # A tibble: 134 x 3
    ##    aisle_id aisle                              n
    ##       <int> <chr>                          <int>
    ##  1       83 fresh vegetables              150609
    ##  2       24 fresh fruits                  150473
    ##  3      123 packaged vegetables fruits     78493
    ##  4      120 yogurt                         55240
    ##  5       21 packaged cheese                41699
    ##  6      115 water seltzer sparkling water  36617
    ##  7       84 milk                           32644
    ##  8      107 chips pretzels                 31269
    ##  9       91 soy lactosefree                26240
    ## 10      112 bread                          23635
    ## # ... with 124 more rows

There are 134 aisles total and top three aisles where the most products
ordered came from were fresh vegetables, fresh fruits, and packaged
vegetables fruits in order.

## Aisle vs product plot

Filtered to only have aisle data with more than 10,000 products ordered.
Then created a plot for how many products have been ordered from each
aisle.

``` r
aisle_df =
  instacart %>%
  count(aisle_id) %>%
  filter(n > 10000) %>%
  rename(number_ordered = n) %>%
  ggplot(aes(x = aisle_id, y = number_ordered)) +
  geom_point() +
  labs(title = "Products Ordered from Each Aisle with at least 10,000 Purchased")
  scale_x_continuous(
    breaks = c(0, 25, 50, 75, 100, 125))
```

    ## <ScaleContinuousPosition>
    ##  Range:  
    ##  Limits:    0 --    1

There were two aisles with significantly higher purchased products with
approximately over 140,000 counts. However, generally most aisles had
under 40,000 purchased products.

## Popular items per aisle

Created data frame for each aisle, baking ingredient, dog food care, and
packaged vegetables fruits. Filtered to aisle specific data and counted
how many times each products were ordered. Arranged data in descending
order to get top 3 popularly ordered products. Then joined all three
data frames and created a table representing the data.

``` r
bake_ingr_df =
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "baking ingredients") %>%
  count(aisle, product_name, name = "count_baking") %>%
  arrange(desc(count_baking)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "baking ingredients")

dog_food_care_df = 
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "dog food care") %>%
  count(aisle, product_name, name = "count_dog") %>%
  arrange(desc(count_dog)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "dog food care")

pack_veg_fruit_df = 
  instacart %>%
  select(product_name, aisle) %>%
  filter(aisle == "packaged vegetables fruits") %>%
  count(aisle, product_name, name = "count_packaged") %>%
  arrange(desc(count_packaged)) %>%
  head(3) %>%
  mutate(rank = c(1, 2, 3)) %>%
  pivot_wider(
    names_from = "aisle",
    values_from = "product_name"
  ) %>%
  relocate(rank, "packaged vegetables fruits")

popular_table_df = 
  left_join(bake_ingr_df, left_join(dog_food_care_df, pack_veg_fruit_df, by = "rank"), 
            by = "rank") %>%
  janitor::clean_names()

knitr::kable(popular_table_df, caption = "Top 3 Products Ordered from Selected Aisles")
```

| rank | baking\_ingredients | count\_baking | dog\_food\_care                               | count\_dog | packaged\_vegetables\_fruits | count\_packaged |
|-----:|:--------------------|--------------:|:----------------------------------------------|-----------:|:-----------------------------|----------------:|
|    1 | Light Brown Sugar   |           499 | Snack Sticks Chicken & Rice Recipe Dog Treats |         30 | Organic Baby Spinach         |            9784 |
|    2 | Pure Baking Soda    |           387 | Organix Chicken & Brown Rice Recipe           |         28 | Organic Raspberries          |            5546 |
|    3 | Cane Sugar          |           336 | Small Dog Biscuits                            |         26 | Organic Blueberries          |            4966 |

Top 3 Products Ordered from Selected Aisles

The top three products purchased from the baking ingredients were light
brown sugar, pure baking soda, and cane sugar. The top three products
purchased from the dog food care aisle were snack sticks chicken & rice
recipe dog treats, organix chicken & brown rice recipe, and small dog
biscuits. Finally the top three most purchased products from the
packaged vegetables fruits were organic baby spinach, organic
raspberries, and organic blueberries. The counts for the top three
products purchased from the packaged vegetables fruits aisle were
significantly higher as compared to the other two aisles.

## Mean hour a product was ordered each day

Filtered data for each product, pink lady apple and coffee ice cream.
Grouped filtered data based on day of the week (0 to 6, what specific
day each number represents not specified) and created new variable for
mean hour each product was ordered. Then created a table for each pink
lady apple and coffee ice cream to represent the data.

``` r
pink_lady_apple =
  instacart %>%
  select("day" = "order_dow", order_hour_of_day, product_name) %>%
  filter(product_name == "Pink Lady Apples") %>%
  group_by(day) %>%
  summarize(mean_order_hour = mean(order_hour_of_day))
knitr::kable(pink_lady_apple, caption = "Mean Hour of Day Pink Lady Apples are Ordered")
```

| day | mean\_order\_hour |
|----:|------------------:|
|   0 |          13.44118 |
|   1 |          11.36000 |
|   2 |          11.70213 |
|   3 |          14.25000 |
|   4 |          11.55172 |
|   5 |          12.78431 |
|   6 |          11.93750 |

Mean Hour of Day Pink Lady Apples are Ordered

``` r
coffee_ice_cream =
  instacart %>%
  select("day" = "order_dow", order_hour_of_day, product_name) %>%
  filter(product_name == "Coffee Ice Cream") %>%
  group_by(day) %>%
  summarize(mean_order_hour = mean(order_hour_of_day))
knitr::kable(coffee_ice_cream, caption = "Mean Hour of Day Coffee Ice Creams are Ordered")
```

| day | mean\_order\_hour |
|----:|------------------:|
|   0 |          13.77419 |
|   1 |          14.31579 |
|   2 |          15.38095 |
|   3 |          15.31818 |
|   4 |          15.21739 |
|   5 |          12.26316 |
|   6 |          13.83333 |

Mean Hour of Day Coffee Ice Creams are Ordered

The mean hour of the day coffee ice creams were ordered were generally
later than the pink lady apple purchases.

# Problem 2

## Load BRFSS data

``` r
data(brfss_smart2010)
```

## Clean BRFSS

Filtered data to only see overall health topics. Reassigned the 5
responses that goes from poor to excellent to be an ordered factors.

``` r
overall_health =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"),
                           ordered = TRUE)) %>%
  rename("state" = "locationabbr", "county" = "locationdesc")
```

## States with &gt;= 7 observed locations in 2002 and 2010

Filtered data to year specific, 2002 and 2010 each. Counted how many
observations each state had but divided results by 5 since observations
from each county were divided based on the 5 responses. Then filtered
data to only see states with at least 7 locations.

``` r
nrow(
overall_health %>%
  filter(year == 2002) %>%
  count(state) %>%
  mutate(n = n/5) %>%
  filter(n >= 7))
```

    ## [1] 6

``` r
nrow(
  overall_health %>%
  filter(year == 2010) %>%
  count(state) %>%
  mutate(n = n/5) %>%
  filter(n >= 7))
```

    ## [1] 14

In 2002, 6 states observed 7 or more locations. In 2010, 14 states
observed 7 or more locations.

## Excellent response data frame and plot

Filtered data to only show observations with response excellent. Added
new variable to represent mean data value across different locations
within the states. Then created a plot to observe to pattern for each
state.

``` r
excellent_df =
  overall_health %>%
  filter(response == "Excellent") %>%
  select(year, state, data_value) %>%
  group_by(year, state) %>%
  summarize(mean_data_value = mean(data_value)) %>%
  drop_na(mean_data_value)
```

    ## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.

``` r
excellent_df %>%
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_point() +
  geom_line() +
  labs(title = "Mean Excellent Responses Across Different Locations Within Each State")
```

![](p8105_hw3_yo2336_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

The mean values from 2002 to 2010 generally decreased very slightly over
time for each states. With some exceptions, the mean values range from
15 to 30 approximately.

## Plot for NY responses to overall health in 2006 and 2010

Filtered to see data for New York state only and for years 2006 and
2010. Then created a plot showing the distribution of the responses per
year.

``` r
overall_health %>%
  filter(state == "NY", year == c(2006, 2010)) %>%
  select(year, state, response, data_value) %>%
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
  labs(title = "Distribution of Responses to Overall Health in NY")
```

    ## Warning in year == c(2006, 2010): longer object length is not a multiple of
    ## shorter object length

![](p8105_hw3_yo2336_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

Generally, there were more observations for the responses good and very
good during the year 2010 compared to 2006.There were less lower level
responses, poor and fair, observed as compared to higher responses for
both years.

# Problem 3

## Load and clean data

Loaded csv data and cleaned. Added new variable to assign whether it was
a weekday or weekend.

``` r
accel_df = 
  read_csv("accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(week_type = case_when(
    day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
    day %in% c("Saturday", "Sunday") ~ "weekend")) %>%
  relocate(week, day_id, day, week_type)
```

    ## Rows: 35 Columns: 1443

    ## -- Column specification --------------------------------------------------------
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## i Use `spec()` to retrieve the full column specification for this data.
    ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
nrow(accel_df)
```

    ## [1] 35

``` r
ncol(accel_df)
```

    ## [1] 1444

The accelerometer data has total of 1444 variables for week, day, and
activity per minute (total 1440 for each minute) the 24 hour period each
observed day. There are 35 total observations, total 35 days observed.

## Total activity for each day

Obtained sum across all minute mark activity after grouping by day. Then
created a table to represent the data.

``` r
total_activity_df = 
  accel_df %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(across(starts_with("activity"))))
```

    ## `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.

``` r
knitr::kable(total_activity_df, caption = "Total Activity for Each Day")
```

| week | day       | total\_activity |
|-----:|:----------|----------------:|
|    1 | Friday    |       480542.62 |
|    1 | Monday    |        78828.07 |
|    1 | Saturday  |       376254.00 |
|    1 | Sunday    |       631105.00 |
|    1 | Thursday  |       355923.64 |
|    1 | Tuesday   |       307094.24 |
|    1 | Wednesday |       340115.01 |
|    2 | Friday    |       568839.00 |
|    2 | Monday    |       295431.00 |
|    2 | Saturday  |       607175.00 |
|    2 | Sunday    |       422018.00 |
|    2 | Thursday  |       474048.00 |
|    2 | Tuesday   |       423245.00 |
|    2 | Wednesday |       440962.00 |
|    3 | Friday    |       467420.00 |
|    3 | Monday    |       685910.00 |
|    3 | Saturday  |       382928.00 |
|    3 | Sunday    |       467052.00 |
|    3 | Thursday  |       371230.00 |
|    3 | Tuesday   |       381507.00 |
|    3 | Wednesday |       468869.00 |
|    4 | Friday    |       154049.00 |
|    4 | Monday    |       409450.00 |
|    4 | Saturday  |         1440.00 |
|    4 | Sunday    |       260617.00 |
|    4 | Thursday  |       340291.00 |
|    4 | Tuesday   |       319568.00 |
|    4 | Wednesday |       434460.00 |
|    5 | Friday    |       620860.00 |
|    5 | Monday    |       389080.00 |
|    5 | Saturday  |         1440.00 |
|    5 | Sunday    |       138421.00 |
|    5 | Thursday  |       549658.00 |
|    5 | Tuesday   |       367824.00 |
|    5 | Wednesday |       445366.00 |

Total Activity for Each Day

When looking at the table, there is no obvious pattern but potentially
lower activity counts during some of the weekend days.

## Plot of activity over 24 hour each day

Used the sums across all 1440 activity interval and created a graph
observing total activity over all 35 observed days and color coded by
day of the week.

``` r
accel_df %>%
  group_by(day_id, day) %>%
  summarize(total_activity = sum(across(starts_with("activity")))) %>%
  ggplot(aes(x = day_id, y = total_activity, color = day)) +
  geom_point() +
  geom_line() +
  labs(title = "24-hour Activity for Each Day",
       x = "Day",
       y = "Total Activity")
```

    ## `summarise()` has grouped output by 'day_id'. You can override using the `.groups` argument.

![](p8105_hw3_yo2336_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

The observations for Saturdays eventually decreased to have the lowest
values over the entire 35 observed days. Though values decreased soon
after a peak, the third Monday had the highest total activity count.
Sundays had overall decreased activity count over the course of 35 days
while Tuesdays and Wednesdays had consistent total counts over 35 days.
